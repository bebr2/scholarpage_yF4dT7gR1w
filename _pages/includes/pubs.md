# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025 Main</div><img src='images/EditCoT/500x300(1).png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Knowledge Editing through Chain-of-Thought](https://arxiv.org/abs/2412.17727)

**Changyue Wang**, Weihang Su, Qingyao Ai, Yichen Tang, Yiqun Liu

[**Code**](https://github.com/bebr2/EditCoT) <strong><span class='show_paper_citations' data='iHDgZ04AAAAJ:ufrVoPGSRksC'></span></strong>
- EditCoT is a novel knowledge editing framework that updates LLMs through iterative chain-of-thought refinement, enabling efficient integration of new knowledge without retraining. It achieves state-of-the-art performance across diverse tasks and languages, offering superior generalization, stability, and effectiveness.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025 Findings</div><img src='images/DecKER/1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://aclanthology.org/2025.findings-acl.1260/)

**Changyue Wang**, Weihang Su, Qingyao Ai, Yujia Zhou, Yiqun Liu

[**Code**](https://github.com/bebr2/DecKER) <strong><span class='show_paper_citations' data='iHDgZ04AAAAJ:UebtZRa9Y70C'></span></strong>
- DecKER is a novel in-context editing framework that decouples reasoning from knowledge injection, mitigating conflicts between updated and original knowledge. It achieves significant improvements in multi-hop reasoning by preserving reasoning integrity while efficiently integrating new knowledge.
</div>
</div>

- [Parametric Retrieval Augmented Generation](https://dl.acm.org/doi/abs/10.1145/3726302.3729957), Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, **Changyue Wang**, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu. ***SIGIR 2025***
- [Pre-training for Legal Case Retrieval Based on Inter-Case Distinctions](https://dl.acm.org/doi/full/10.1145/3735127), Weihang Su, Qingyao Ai, Yueyue Wu, Anzhe Xie, **Changyue Wang**, Yixiao Ma, Haitao Li, Zhijing Wu, Yiqun Liu, Min Zhang. ***ACM TOIS***